{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a991c593-90da-4f1a-891f-dbf26e192225",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1.Handling numerical data and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cbe401-e380-460d-a9e2-f69037e7d21d",
   "metadata": {},
   "source": [
    "Handling numerical data in a machine learning model involves preprocessing steps that help improve model performance and ensure that the data is in a suitable format for training. Here’s a detailed lesson on how to handle numerical data using Python libraries:\r\n",
    "\r\n",
    "### Preprocessing Numerical Data\r\n",
    "\r\n",
    "1. **Missing Values Handling:**\r\n",
    "   - Missing values are common in real-world datasets. You can handle them by:\r\n",
    "     - Removing rows or columns with missing values if they are few and won’t significantly impact the dataset's integrity.\r\n",
    "     - Imputing missing values using strategies like mean, median, mode, or using advanced techniques like K-nearest neighbors (KNN) imputation.\r\n",
    "     - Libraries like pandas (`df.dropna()`, `df.fillna()`) and scikit-learn (`SimpleImputer`) offer methods for handling missing values.\r\n",
    "\r\n",
    "2. **Scaling and Normalization:**\r\n",
    "   - Scaling ensures that all numerical features have the same scale, preventing features with larger scales from dominating during training. Common scaling techniques include:\r\n",
    "     - Min-Max Scaling: Scales features to a specified range (e.g., 0 to 1).\r\n",
    "     - Standardization (Z-score Scaling): Scales features to have a mean of 0 and a standard deviation of 1.\r\n",
    "   - Libraries like scikit-learn (`MinMaxScaler`, `StandardScaler`) provide functions for scaling numerical features.\r\n",
    "\r\n",
    "3. **Feature Engineering:**\r\n",
    "   - Feature engineering involves creating new features or transforming existing ones to improve model performance. Techniques include:\r\n",
    "     - Polynomial Features: Generating higher-degree polynomial features from existing features.\r\n",
    "     - Logarithmic or Exponential Transformations: Applying log or exponential functions to features to handle skewed distributions.\r\n",
    "   - Scikit-learn (`PolynomialFeatures`) and numpy (`np.log()`, `np.exp()`) are useful for feature engineering.\r\n",
    "\r\n",
    "4. **Outlier Detection and Handling:**\r\n",
    "   - Outliers can skew model predictions. Techniques to handle outliers include:\r\n",
    "     - Identifying outliers using statistical methods like z-scores or IQR (Interquartile Range).\r\n",
    "     - Handling outliers by trimming, winsorizing, or transforming the data.\r\n",
    "   - Libraries like scipy (`scipy.stats.zscore`, `scipy.stats.iqr`) and pandas (`df.clip()`, `df.transform()`) can be used for outlier detection and handling.\r\n",
    "\r\n",
    "### Example Code Snippets (Python with pandas and scikit-learn)\r\n",
    "\r\n",
    "```python\r\n",
    "import pandas as pd\r\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures, RobustScaler\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "from scipy.stats import zscore, iqr\r\n",
    "\r\n",
    "# Sample DataFrame\r\n",
    "data = {'Age': [25, 30, 35, None, 40],\r\n",
    "        'Income': [50000, 60000, None, 70000, 80000]}\r\n",
    "\r\n",
    "df = pd.DataFrame(data)\r\n",
    "\r\n",
    "# Handling Missing Values\r\n",
    "imputer = SimpleImputer(strategy='mean')\r\n",
    "df[['Age', 'Income']] = imputer.fit_transform(df[['Age', 'Income']])\r\n",
    "\r\n",
    "# Scaling and Normalization\r\n",
    "min_max_scaler = MinMaxScaler()\r\n",
    "standard_scaler = StandardScaler()\r\n",
    "robust_scaler = RobustScaler()\r\n",
    "\r\n",
    "df['Age_MinMaxScaled'] = min_max_scaler.fit_transform(df[['Age']])\r\n",
    "df['Income_StandardScaled'] = standard_scaler.fit_transform(df[['Income']])\r\n",
    "df['Income_RobustScaled'] = robust_scaler.fit_transform(df[['Income']])\r\n",
    "\r\n",
    "# Feature Engineering\r\n",
    "poly_features = PolynomialFeatures(degree=2)\r\n",
    "df_poly = poly_features.fit_transform(df[['Age', 'Income']])\r\n",
    "df_poly = pd.DataFrame(df_poly, columns=poly_features.get_feature_names(['Age', 'Income']))\r\n",
    "df = pd.concat([df, df_poly], axis=1)\r\n",
    "\r\n",
    "# Outlier Detection and Handling\r\n",
    "df['Age_Zscore'] = zscore(df['Age'])\r\n",
    "df['Income_IQR'] = iqr(df['Income'])\r\n",
    "```\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "Handling numerical data involves several preprocessing steps such as handling missing values, scaling and normalization, feature engineering, and outlier detection and handling. Python libraries like pandas, scikit-learn, and scipy offer efficient functions and methods to perform these tasks effectively. Understanding these preprocessing techniques and choosing the appropriate ones for your dataset can significantly improve the performance of your machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3236d6-b803-4fb6-9766-e6d8318ab6db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2.Aiming libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d440f4b8-3911-4787-9037-ea82602cf0a5",
   "metadata": {},
   "source": [
    "Several Python libraries offer functions and tools for handling and extracting features from numerical data. Here are some of the most commonly used libraries and functions for these tasks:\r\n",
    "\r\n",
    "### Libraries for Handling Numerical Data:\r\n",
    "\r\n",
    "1. **pandas**:\r\n",
    "   - `dropna()`: Drops rows or columns with missing values.\r\n",
    "   - `fillna()`: Fills missing values with specified values.\r\n",
    "   - `clip()`: Clips values to a specified range.\r\n",
    "   - `transform()`: Applies a function element-wise to the data.\r\n",
    "   - `rolling()`: Performs rolling window calculations.\r\n",
    "\r\n",
    "2. **scikit-learn**:\r\n",
    "   - `SimpleImputer`: Imputes missing values using strategies like mean, median, mode, etc.\r\n",
    "   - `MinMaxScaler`: Scales features to a specified range.\r\n",
    "   - `StandardScaler`: Standardizes features by removing the mean and scaling to unit variance.\r\n",
    "   - `RobustScaler`: Scales features using robust statistics to handle outliers.\r\n",
    "   - `PolynomialFeatures`: Generates polynomial features.\r\n",
    "   - `FunctionTransformer`: Applies a specified function to the data.\r\n",
    "\r\n",
    "3. **numpy**:\r\n",
    "   - `np.log()`, `np.exp()`: Logarithmic and exponential transformations.\r\n",
    "   - `np.clip()`: Clips values to a specified range.\r\n",
    "   - `np.percentile()`: Calculates percentiles to detect outliers.\r\n",
    "   - `np.where()`: Conditional element-wise operation.\r\n",
    "\r\n",
    "4. **scipy**:\r\n",
    "   - `scipy.stats.zscore()`: Computes z-scores for outlier detection.\r\n",
    "   - `scipy.stats.iqr()`: Computes the interquartile range for outlier detection.\r\n",
    "\r\n",
    "5. **statsmodels**:\r\n",
    "   - `statsmodels.api`: Provides statistical functions and models for data analysis.\r\n",
    "\r\n",
    "### Libraries for Feature Extraction from Numerical Data:\r\n",
    "\r\n",
    "1. **scikit-learn**:\r\n",
    "   - `SelectKBest`, `SelectPercentile`: Selects the k best or percentile of features based on statistical tests.\r\n",
    "   - `VarianceThreshold`: Removes low-variance features.\r\n",
    "   - `PCA (Principal Component Analysis)`: Reduces dimensionality by transforming features into principal components.\r\n",
    "   - `RFE (Recursive Feature Elimination)`: Selects features by recursively considering smaller and smaller sets of features.\r\n",
    "\r\n",
    "2. **feature-engine**:\r\n",
    "   - Provides various transformers for feature engineering tasks like discretization, encoding, variable selection, etc.\r\n",
    "\r\n",
    "3. **statsmodels**:\r\n",
    "   - `statsmodels.api.OLS`: Performs Ordinary Least Squares regression for feature selection.\r\n",
    "   - `statsmodels.api.Logit`: Fits a logistic regression model for feature selection in classification tasks.\r\n",
    "\r\n",
    "4. **xgboost**, **LightGBM**, **CatBoost**:\r\n",
    "   - Gradient boosting libraries that offer feature importance methods for tree-based models.\r\n",
    "\r\n",
    "### Example Code Snippets (Using Libraries for Handling and Feature Extraction):\r\n",
    "\r\n",
    "```python\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\r\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\r\n",
    "\r\n",
    "# Sample DataFrame\r\n",
    "data = {'Age': [25, 30, 35, None, 40],\r\n",
    "        'Income': [50000, 60000, None, 70000, 80000]}\r\n",
    "\r\n",
    "df = pd.DataFrame(data)\r\n",
    "\r\n",
    "# Handling Missing Values\r\n",
    "imputer = SimpleImputer(strategy='mean')\r\n",
    "df[['Age', 'Income']] = imputer.fit_transform(df[['Age', 'Income']])\r\n",
    "\r\n",
    "# Scaling and Normalization\r\n",
    "min_max_scaler = MinMaxScaler()\r\n",
    "df['Age_MinMaxScaled'] = min_max_scaler.fit_transform(df[['Age']])\r\n",
    "\r\n",
    "# Feature Engineering\r\n",
    "poly_features = PolynomialFeatures(degree=2)\r\n",
    "df_poly = poly_features.fit_transform(df[['Age', 'Income']])\r\n",
    "df_poly = pd.DataFrame(df_poly, columns=poly_features.get_feature_names(['Age', 'Income']))\r\n",
    "df = pd.concat([df, df_poly], axis=1)\r\n",
    "\r\n",
    "# Feature Selection\r\n",
    "selector = SelectKBest(score_func=f_regression, k=1)\r\n",
    "selected_features = selector.fit_transform(df[['Age', 'Income']], df['Age'])\r\n",
    "```\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "These libraries and functions provide a comprehensive set of tools for handling numerical data, including missing value imputation, scaling, normalization, feature engineering, and feature selection. By leveraging these libraries effectively, you can preprocess and extract meaningful features from your numerical data to improve the performance of your machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84711cf3-0664-41be-ab60-154c4f3253d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3.practical implementation for Aiming libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2291b6ff-bdb9-4d60-95aa-db9451577484",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\r\n",
    "### pandas:\r\n",
    "\r\n",
    "```python\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Sample DataFrame\r\n",
    "data = {'Age': [25, 30, 35, None, 40],\r\n",
    "        'Income': [50000, 60000, None, 70000, 80000]}\r\n",
    "df = pd.DataFrame(data)\r\n",
    "\r\n",
    "# Drop rows with missing values\r\n",
    "df_dropped = df.dropna()\r\n",
    "\r\n",
    "# Fill missing values with mean\r\n",
    "df_filled = df.fillna(df.mean())\r\n",
    "\r\n",
    "# Clip values to a specified range\r\n",
    "df_clipped = df.clip(lower=20, upper=50)\r\n",
    "\r\n",
    "# Apply a function element-wise to the data\r\n",
    "df_transformed = df['Age'].transform(lambda x: x + 10)\r\n",
    "\r\n",
    "# Perform rolling window calculations\r\n",
    "df_rolling = df.rolling(window=2).mean()\r\n",
    "\r\n",
    "print(df_dropped)\r\n",
    "print(df_filled)\r\n",
    "print(df_clipped)\r\n",
    "print(df_transformed)\r\n",
    "print(df_rolling)\r\n",
    "```\r\n",
    "\r\n",
    "### scikit-learn (SimpleImputer, MinMaxScaler, PolynomialFeatures):\r\n",
    "\r\n",
    "```python\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\r\n",
    "\r\n",
    "# Handling Missing Values with SimpleImputer\r\n",
    "imputer = SimpleImputer(strategy='mean')\r\n",
    "df[['Age', 'Income']] = imputer.fit_transform(df[['Age', 'Income']])\r\n",
    "\r\n",
    "# Scaling and Normalization with MinMaxScaler\r\n",
    "min_max_scaler = MinMaxScaler()\r\n",
    "df['Age_MinMaxScaled'] = min_max_scaler.fit_transform(df[['Age']])\r\n",
    "\r\n",
    "# Feature Engineering with PolynomialFeatures\r\n",
    "poly_features = PolynomialFeatures(degree=2)\r\n",
    "df_poly = poly_features.fit_transform(df[['Age', 'Income']])\r\n",
    "df_poly = pd.DataFrame(df_poly, columns=poly_features.get_feature_names(['Age', 'Income']))\r\n",
    "df = pd.concat([df, df_poly], axis=1)\r\n",
    "\r\n",
    "print(df.head())\r\n",
    "```\r\n",
    "\r\n",
    "### numpy (np.log, np.exp, np.clip, np.percentile, np.where):\r\n",
    "\r\n",
    "```python\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# Logarithmic and Exponential Transformations\r\n",
    "df['Income_Log'] = np.log(df['Income'])\r\n",
    "df['Income_Exp'] = np.exp(df['Income'])\r\n",
    "\r\n",
    "# Clip values to a specified range\r\n",
    "df['Age_Clipped'] = np.clip(df['Age'], a_min=20, a_max=50)\r\n",
    "\r\n",
    "# Calculate percentile for outlier detection\r\n",
    "percentile_95 = np.percentile(df['Income'], 95)\r\n",
    "\r\n",
    "# Conditional element-wise operation\r\n",
    "df['Income_Above_Threshold'] = np.where(df['Income'] > percentile_95, 'High', 'Low')\r\n",
    "\r\n",
    "print(df.head())\r\n",
    "```\r\n",
    "\r\n",
    "### scipy (scipy.stats.zscore, scipy.stats.iqr):\r\n",
    "\r\n",
    "```python\r\n",
    "from scipy.stats import zscore, iqr\r\n",
    "\r\n",
    "# Calculate z-scores for outlier detection\r\n",
    "df['Age_Zscore'] = zscore(df['Age'])\r\n",
    "\r\n",
    "# Calculate interquartile range (IQR) for outlier detection\r\n",
    "iqr_value = iqr(df['Income'])\r\n",
    "\r\n",
    "print(df.head())\r\n",
    "```\r\n",
    "\r\n",
    "These examples demonstrate the usage of various functions and methods from pandas, scikit-learn, numpy, and scipy for handling numerical data, including missing value imputation, scaling, normalization, feature engineering, and outlier detection.s from your numerical data to improve the performance of your machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
