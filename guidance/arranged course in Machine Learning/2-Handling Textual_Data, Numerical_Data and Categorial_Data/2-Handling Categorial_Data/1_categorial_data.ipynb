{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c62b784e-d859-45ae-91ef-70830209ed22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1.techniques of handling categorial_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09517e2-01f5-4f18-9b07-514d38f6344d",
   "metadata": {},
   "source": [
    "Handling categorical data is an essential part of preprocessing before training machine learning models. Categorical data refers to variables that contain label values rather than numerical values. To use categorical data in machine learning models, it must be converted into a numerical format. There are several methods to achieve this, including one-hot encoding, label encoding, binary encoding, target encoding, and using embeddings. Below, we will discuss each method in detail and provide examples of how to implement them using Python libraries like pandas and scikit-learn.\r\n",
    "\r\n",
    "### 1. One-Hot Encoding\r\n",
    "\r\n",
    "One-hot encoding is a technique that converts categorical values into a binary matrix. Each category is represented by a binary vector, where only the corresponding category's position is marked with a 1, and all others are 0.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Sample data\r\n",
    "data = {'color': ['red', 'blue', 'green', 'blue', 'red']}\r\n",
    "df = pd.DataFrame(data)\r\n",
    "\r\n",
    "# One-hot encoding using pandas\r\n",
    "df_encoded = pd.get_dummies(df, columns=['color'])\r\n",
    "print(df_encoded)\r\n",
    "```\r\n",
    "\r\n",
    "**Output**:\r\n",
    "```\r\n",
    "   color_blue  color_green  color_red\r\n",
    "0           0            0          1\r\n",
    "1           1            0          0\r\n",
    "2           0            1          0\r\n",
    "3           1            0          0\r\n",
    "4           0            0          1\r\n",
    "```\r\n",
    "\r\n",
    "### 2. Label Encoding\r\n",
    "\r\n",
    "Label encoding assigns a unique integer to each category. This method can introduce ordinal relationships between categories that do not naturally exist, which may not be suitable for all algorithms.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "\r\n",
    "# Sample data\r\n",
    "data = {'color': ['red', 'blue', 'green', 'blue', 'red']}\r\n",
    "df = pd.DataFrame(data)\r\n",
    "\r\n",
    "# Label encoding\r\n",
    "encoder = LabelEncoder()\r\n",
    "df['color_encoded'] = encoder.fit_transform(df['color'])\r\n",
    "print(df)\r\n",
    "```\r\n",
    "\r\n",
    "**Output**:\r\n",
    "```\r\n",
    "   color  color_encoded\r\n",
    "0    red              2\r\n",
    "1   blue              0\r\n",
    "2  green              1\r\n",
    "3   blue              0\r\n",
    "4    red              2\r\n",
    "```\r\n",
    "\r\n",
    "### 3. Binary Encoding\r\n",
    "\r\n",
    "Binary encoding converts categories into binary numbers, and each binary digit is then turned into a separate column.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "!pip install category_encoders\r\n",
    "import pandas as pd\r\n",
    "import category_encoders as ce\r\n",
    "\r\n",
    "# Sample data\r\n",
    "data = {'color': ['red', 'blue', 'green', 'blue', 'red']}\r\n",
    "df = pd.DataFrame(data)\r\n",
    "\r\n",
    "# Binary encoding\r\n",
    "encoder = ce.BinaryEncoder(cols=['color'])\r\n",
    "df_encoded = encoder.fit_transform(df)\r\n",
    "print(df_encoded)\r\n",
    "```\r\n",
    "\r\n",
    "**Output**:\r\n",
    "```\r\n",
    "   color_0  color_1\r\n",
    "0        1        0\r\n",
    "1        0        0\r\n",
    "2        0        1\r\n",
    "3        0        0\r\n",
    "4        1        0\r\n",
    "```\r\n",
    "\r\n",
    "### 4. Target Encoding\r\n",
    "\r\n",
    "Target encoding replaces each category with the mean of the target variable for that category. This technique is useful for categorical variables with high cardinality.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "import pandas as pd\r\n",
    "import category_encoders as ce\r\n",
    "\r\n",
    "# Sample data\r\n",
    "data = {\r\n",
    "    'color': ['red', 'blue', 'green', 'blue', 'red'],\r\n",
    "    'target': [1, 0, 1, 1, 0]\r\n",
    "}\r\n",
    "df = pd.DataFrame(data)\r\n",
    "\r\n",
    "# Target encoding\r\n",
    "encoder = ce.TargetEncoder(cols=['color'])\r\n",
    "df['color_encoded'] = encoder.fit_transform(df['color'], df['target'])\r\n",
    "print(df)\r\n",
    "```\r\n",
    "\r\n",
    "**Output**:\r\n",
    "```\r\n",
    "   color  target  color_encoded\r\n",
    "0    red       1       0.500000\r\n",
    "1   blue       0       0.500000\r\n",
    "2  green       1       1.000000\r\n",
    "3   blue       1       0.500000\r\n",
    "4    red       0       0.500000\r\n",
    "```\r\n",
    "\r\n",
    "### 5. Using Embeddings\r\n",
    "\r\n",
    "For high cardinality categorical variables, embeddings can be used, especially in neural networks. Embeddings convert categories into dense vectors of fixed size.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Embedding, Flatten, Dense\r\n",
    "\r\n",
    "# Sample data\r\n",
    "data = {'color': ['red', 'blue', 'green', 'blue', 'red']}\r\n",
    "df = pd.DataFrame(data)\r\n",
    "\r\n",
    "# Convert categories to numeric\r\n",
    "encoder = LabelEncoder()\r\n",
    "df['color_encoded'] = encoder.fit_transform(df['color'])\r\n",
    "\r\n",
    "# Define the model\r\n",
    "model = Sequential()\r\n",
    "model.add(Embedding(input_dim=len(df['color'].unique()), output_dim=8, input_length=1))\r\n",
    "model.add(Flatten())\r\n",
    "model.add(Dense(1, activation='sigmoid'))\r\n",
    "\r\n",
    "# Compile the model\r\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\r\n",
    "\r\n",
    "# Print model summary\r\n",
    "print(model.summary())\r\n",
    "\r\n",
    "# Sample target variable\r\n",
    "df['target'] = [1, 0, 1, 1, 0]\r\n",
    "\r\n",
    "# Train the model\r\n",
    "X = df['color_encoded'].values\r\n",
    "y = df['target'].values\r\n",
    "model.fit(X, y, epochs=10, verbose=1)\r\n",
    "```\r\n",
    "\r\n",
    "**Output**:\r\n",
    "```\r\n",
    "Model: \"sequential\"\r\n",
    "_________________________________________________________________\r\n",
    "Layer (type)                 Output Shape              Param #   \r\n",
    "=================================================================\r\n",
    "embedding (Embedding)        (None, 1, 8)              24        \r\n",
    "_________________________________________________________________\r\n",
    "flatten (Flatten)            (None, 8)                 0         \r\n",
    "_________________________________________________________________\r\n",
    "dense (Dense)                (None, 1)                 9         \r\n",
    "=================================================================\r\n",
    "Total params: 33\r\n",
    "Trainable params: 33\r\n",
    "Non-trainable params: 0\r\n",
    "_________________________________________________________________\r\n",
    "```\r\n",
    "\r\n",
    "### Handling Multiple Categorical Columns\r\n",
    "\r\n",
    "When dealing with multiple categorical columns, you can apply these encoding techniques to each column separately.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "import pandas as pd\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "\r\n",
    "# Sample data\r\n",
    "data = {\r\n",
    "    'color': ['red', 'blue', 'green', 'blue', 'red'],\r\n",
    "    'size': ['S', 'M', 'L', 'M', 'S']\r\n",
    "}\r\n",
    "df = pd.DataFrame(data)\r\n",
    "\r\n",
    "# One-hot encoding\r\n",
    "encoder = OneHotEncoder(sparse=False)\r\n",
    "encoded_columns = encoder.fit_transform(df[['color', 'size']])\r\n",
    "encoded_df = pd.DataFrame(encoded_columns, columns=encoder.get_feature_names_out(['color', 'size']))\r\n",
    "df = pd.concat([df, encoded_df], axis=1).drop(['color', 'size'], axis=1)\r\n",
    "print(df)\r\n",
    "```\r\n",
    "\r\n",
    "**Output**:\r\n",
    "```\r\n",
    "   color_blue  color_green  color_red  size_L  size_M  size_S\r\n",
    "0         0.0          0.0        1.0     0.0     0.0     1.0\r\n",
    "1         1.0          0.0        0.0     0.0     1.0     0.0\r\n",
    "2         0.0          1.0        0.0     1.0     0.0     0.0\r\n",
    "3         1.0          0.0        0.0     0.0     1.0     0.0\r\n",
    "4         0.0          0.0        1.0     0.0     0.0     1.0\r\n",
    "```\r\n",
    "\r\n",
    "### Inserting Encoded Data into a Model\r\n",
    "\r\n",
    "Once the categorical data is encoded, it can be inserted into a machine learning model like any other numerical data.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "# Sample data\r\n",
    "data = {\r\n",
    "    'color': ['red', 'blue', 'green', 'blue', 'red'],\r\n",
    "    'size': ['S', 'M', 'L', 'M', 'S'],\r\n",
    "    'target': [1, 0, 1, 1, 0]\r\n",
    "}\r\n",
    "df = pd.DataFrame(data)\r\n",
    "\r\n",
    "# One-hot encoding\r\n",
    "encoder = OneHotEncoder(sparse=False)\r\n",
    "encoded_columns = encoder.fit_transform(df[['color', 'size']])\r\n",
    "encoded_df = pd.DataFrame(encoded_columns, columns=encoder.get_feature_names_out(['color', 'size']))\r\n",
    "df = pd.concat([df, encoded_df], axis=1).drop(['color', 'size'], axis=1)\r\n",
    "\r\n",
    "# Split data into training and testing sets\r\n",
    "X = df.drop('target', axis=1)\r\n",
    "y = df['target']\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n",
    "\r\n",
    "# Train a Random Forest classifier\r\n",
    "model = RandomForestClassifier()\r\n",
    "model.fit(X_train, y_train)\r\n",
    "\r\n",
    "# Predict and evaluate the model\r\n",
    "y_pred = model.predict(X_test)\r\n",
    "accuracy = accuracy_score(y_test, y_pred)\r\n",
    "print('Accuracy:', accuracy)\r\n",
    "```\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "Handling categorical data is crucial for building effective machine learning models. The choice of encoding technique depends on the nature of the data and the model being used. One-hot encoding and label encoding are simple and commonly used methods, while binary encoding, target encoding, and embeddings offer more advanced solutions for handling high-cardinality and complex categorical data. Properly encoding categorical variables ensures that the model can leverage all available information, leading to better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280f696-3b7b-455f-bccf-4824fb03b57d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2.when to use which?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c59a4cb-7b1b-436d-91c4-260f7dff3aa9",
   "metadata": {},
   "source": [
    "Choosing the appropriate method for encoding categorical data depends on several factors, including the nature of the categorical variables, the machine learning algorithm being used, and the specific requirements of the task. Here are guidelines for when to use each encoding technique:\r\n",
    "\r\n",
    "### 1. One-Hot Encoding\r\n",
    "\r\n",
    "**When to use:**\r\n",
    "- When the categorical variable is nominal (no inherent order).\r\n",
    "- When the number of unique categories is relatively small.\r\n",
    "- When you are using machine learning algorithms that can handle high-dimensional sparse data well, such as tree-based methods (e.g., Random Forest, Gradient Boosting).\r\n",
    "\r\n",
    "**Advantages:**\r\n",
    "- Simple and easy to implement.\r\n",
    "- Does not impose any ordinal relationship between categories.\r\n",
    "\r\n",
    "**Disadvantages:**\r\n",
    "- Can lead to a high-dimensional feature space if there are many categories.\r\n",
    "\r\n",
    "**Example Use Case:**\r\n",
    "```python\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Sample data\r\n",
    "data = {'color': ['red', 'blue', 'green', 'blue', 'red']}\r\n",
    "df = pd.DataFrame(data)\r\n",
    "\r\n",
    "# One-hot encoding\r\n",
    "encoder = OneHotEncoder(sparse=False)\r\n",
    "encoded_columns = encoder.fit_transform(df[['color']])\r\n",
    "encoded_df = pd.DataFrame(encoded_columns, columns=encoder.get_feature_names_out(['color']))\r\n",
    "df = pd.concat([df, encoded_df], axis=1).drop(['color'], axis=1)\r\n",
    "```\r\n",
    "\r\n",
    "### 2. Label Encoding\r\n",
    "\r\n",
    "**When to use:**\r\n",
    "- When the categorical variable is ordinal (has an inherent order).\r\n",
    "- When using algorithms that can handle categorical integers and do not assume an ordinal relationship, such as decision trees.\r\n",
    "- When there are no many unique categories (to avoid imposing artificial order).\r\n",
    "\r\n",
    "**Advantages:**\r\n",
    "- Simple and efficient.\r\n",
    "- Does not increase dimensionality.\r\n",
    "\r\n",
    "**Disadvantages:**\r\n",
    "- May introduce ordinal relationships between categories that do not exist, leading to potential model bias.\r\n",
    "\r\n",
    "**Example Use Case:**\r\n",
    "```python\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Sample data\r\n",
    "data = {'color': ['red', 'blue', 'green', 'blue', 'red']}\r\n",
    "df = pd.DataFrame(data)\r\n",
    "\r\n",
    "# Label encoding\r\n",
    "encoder = LabelEncoder()\r\n",
    "df['color_encoded'] = encoder.fit_transform(df['color'])\r\n",
    "```\r\n",
    "\r\n",
    "### 3. Binary Encoding\r\n",
    "\r\n",
    "**When to use:**\r\n",
    "- When the categorical variable has a high number of unique categories.\r\n",
    "- When dimensionality needs to be reduced compared to one-hot encoding but without imposing order.\r\n",
    "- When you are using algorithms that can handle binary features well.\r\n",
    "\r\n",
    "**Advantages:**\r\n",
    "- Reduces dimensionality compared to one-hot encoding.\r\n",
    "- Preserves uniqueness of categories without implying order.\r\n",
    "\r\n",
    "**Disadvantages:**\r\n",
    "- More complex than one-hot or label encoding.\r\n",
    "- May not be as interpretable.\r\n",
    "\r\n",
    "**Example Use Case:**\r\n",
    "```python\r\n",
    "import category_encoders as ce\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Sample data\r\n",
    "data = {'color': ['red', 'blue', 'green', 'blue', 'red']}\r\n",
    "df = pd.DataFrame(data)\r\n",
    "\r\n",
    "# Binary encoding\r\n",
    "encoder = ce.BinaryEncoder(cols=['color'])\r\n",
    "df_encoded = encoder.fit_transform(df)\r\n",
    "```\r\n",
    "\r\n",
    "### 4. Target Encoding\r\n",
    "\r\n",
    "**When to use:**\r\n",
    "- When the categorical variable has a high cardinality.\r\n",
    "- When the relationship between categories and the target variable needs to be captured.\r\n",
    "- When using algorithms that can benefit from more informative features, such as linear models or neural networks.\r\n",
    "\r\n",
    "**Advantages:**\r\n",
    "- Captures the relationship between categorical variable and the target variable.\r\n",
    "- Reduces dimensionality significantly.\r\n",
    "\r\n",
    "**Disadvantages:**\r\n",
    "- Can lead to overfitting if not regularized properly (e.g., using smoothing or cross-validation).\r\n",
    "- Requires target variable, making it unsuitable for unsupervised learning.\r\n",
    "\r\n",
    "**Example Use Case:**\r\n",
    "```python\r\n",
    "import category_encoders as ce\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Sample data\r\n",
    "data = {\r\n",
    "    'color': ['red', 'blue', 'green', 'blue', 'red'],\r\n",
    "    'target': [1, 0, 1, 1, 0]\r\n",
    "}\r\n",
    "df = pd.DataFrame(data)\r\n",
    "\r\n",
    "# Target encoding\r\n",
    "encoder = ce.TargetEncoder(cols=['color'])\r\n",
    "df['color_encoded'] = encoder.fit_transform(df['color'], df['target'])\r\n",
    "```\r\n",
    "\r\n",
    "### 5. Using Embeddings\r\n",
    "\r\n",
    "**When to use:**\r\n",
    "- When dealing with high-cardinality categorical variables in neural networks.\r\n",
    "- When the relationships between categories are complex and can benefit from dense representations.\r\n",
    "- When training deep learning models that can leverage embeddings for efficiency and performance.\r\n",
    "\r\n",
    "**Advantages:**\r\n",
    "- Provides dense, low-dimensional representations.\r\n",
    "- Can capture complex relationships between categories.\r\n",
    "\r\n",
    "**Disadvantages:**\r\n",
    "- Requires more computational resources and tuning.\r\n",
    "- Less interpretable compared to other encoding methods.\r\n",
    "\r\n",
    "**Example Use Case:**\r\n",
    "```python\r\n",
    "import pandas as pd\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Embedding, Flatten, Dense\r\n",
    "\r\n",
    "# Sample data\r\n",
    "data = {'color': ['red', 'blue', 'green', 'blue', 'red']}\r\n",
    "df = pd.DataFrame(data)\r\n",
    "\r\n",
    "# Convert categories to numeric\r\n",
    "encoder = LabelEncoder()\r\n",
    "df['color_encoded'] = encoder.fit_transform(df['color'])\r\n",
    "\r\n",
    "# Define the model\r\n",
    "model = Sequential()\r\n",
    "model.add(Embedding(input_dim=len(df['color'].unique()), output_dim=8, input_length=1))\r\n",
    "model.add(Flatten())\r\n",
    "model.add(Dense(1, activation='sigmoid'))\r\n",
    "\r\n",
    "# Compile the model\r\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\r\n",
    "\r\n",
    "# Sample target variable\r\n",
    "df['target'] = [1, 0, 1, 1, 0]\r\n",
    "\r\n",
    "# Train the model\r\n",
    "X = df['color_encoded'].values\r\n",
    "y = df['target'].values\r\n",
    "model.fit(X, y, epochs=10, verbose=1)\r\n",
    "```\r\n",
    "\r\n",
    "### Summary\r\n",
    "\r\n",
    "- **One-Hot Encoding**: Use for nominal variables with few categories, suitable for tree-based algorithms.\r\n",
    "- **Label Encoding**: Use for ordinal variables or when the algorithm can handle categorical integers.\r\n",
    "- **Binary Encoding**: Use for high-cardinality variables to reduce dimensionality without imposing order.\r\n",
    "- **Target Encoding**: Use for high-cardinality variables when capturing the relationship with the target variable is beneficial.\r\n",
    "- **Embeddings**: Use for high-cardinality variables in deep learning models to capture complex relationships.\r\n",
    "\r\n",
    "Selecting the appropriate encoding method is crucial for improving model performance and interpretability. Consider the nature of the data, the algorithm being used, and the specific requirements of the task when choosing the encoding technique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
