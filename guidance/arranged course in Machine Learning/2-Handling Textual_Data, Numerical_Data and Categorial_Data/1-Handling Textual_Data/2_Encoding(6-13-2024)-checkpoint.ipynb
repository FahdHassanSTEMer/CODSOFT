{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505870d7-8a23-4b92-bfa2-28f6b446cb1e",
   "metadata": {},
   "source": [
    "# Types of Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8e1777-14b6-417c-afbf-e63dbf68445e",
   "metadata": {},
   "source": [
    "### Encoding in NLP (Natural Language Processing)\r\n",
    "\r\n",
    "Encoding is a fundamental step in natural language processing (NLP) that involves transforming text data into numerical representations that can be processed by machine learning models. Since most machine learning algorithms require numerical input, encoding techniques are essential for converting text into formats suitable for analysis and model training.\r\n",
    "\r\n",
    "### Types of Encoding in NLP\r\n",
    "\r\n",
    "1. **One-Hot Encoding**\r\n",
    "2. **Label Encoding**\r\n",
    "3. **Bag-of-Words (BoW)**\r\n",
    "4. **Term Frequency-Inverse Document Frequency (TF-IDF)**\r\n",
    "5. **Word Embeddings (e.g., Word2Vec, GloVe)**\r\n",
    "6. **Character-Level Embeddings**\r\n",
    "7. **Contextualized Word Embeddings (e.g., BERT, GPT)**\r\n",
    "\r\n",
    "### Detailed Explanation of Each Encoding Method\r\n",
    "\r\n",
    "#### 1. One-Hot Encoding\r\n",
    "One-hot encoding represents each word in a vocabulary as a binary vector with a single high (1) value and all other values low (0). This method is simple but can lead to high-dimensional sparse matrices, especially for large vocabularies.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# Sample data\r\n",
    "vocab = ['cat', 'dog', 'fish']\r\n",
    "data = ['cat', 'dog', 'fish', 'cat']\r\n",
    "\r\n",
    "# Fit the one-hot encoder\r\n",
    "encoder = OneHotEncoder(sparse=False)\r\n",
    "encoded = encoder.fit_transform(np.array(data).reshape(-1, 1))\r\n",
    "\r\n",
    "print(encoded)\r\n",
    "# Output: \r\n",
    "# [[1. 0. 0.]\r\n",
    "#  [0. 1. 0.]\r\n",
    "#  [0. 0. 1.]\r\n",
    "#  [1. 0. 0.]]\r\n",
    "```\r\n",
    "\r\n",
    "#### 2. Label Encoding\r\n",
    "Label encoding assigns a unique integer to each word in the vocabulary. This method can introduce an ordinal relationship between words, which is often not meaningful.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "\r\n",
    "# Sample data\r\n",
    "data = ['cat', 'dog', 'fish', 'cat']\r\n",
    "\r\n",
    "# Fit the label encoder\r\n",
    "encoder = LabelEncoder()\r\n",
    "encoded = encoder.fit_transform(data)\r\n",
    "\r\n",
    "print(encoded)\r\n",
    "# Output: [0 1 2 0]\r\n",
    "```\r\n",
    "\r\n",
    "#### 3. Bag-of-Words (BoW)\r\n",
    "Bag-of-Words represents text data by counting the frequency of each word in the document. It results in a sparse matrix where each row represents a document and each column represents a word from the vocabulary.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "\r\n",
    "# Sample data\r\n",
    "documents = [\"cat and dog\", \"dog and fish\", \"cat and fish\"]\r\n",
    "\r\n",
    "# Fit the BoW model\r\n",
    "vectorizer = CountVectorizer()\r\n",
    "X = vectorizer.fit_transform(documents)\r\n",
    "\r\n",
    "print(X.toarray())\r\n",
    "print(vectorizer.get_feature_names_out())\r\n",
    "# Output: \r\n",
    "# [[1 1 0 1]\r\n",
    "#  [0 1 1 1]\r\n",
    "#  [1 0 1 1]]\r\n",
    "# ['and' 'cat' 'dog' 'fish']\r\n",
    "```\r\n",
    "\r\n",
    "#### 4. Term Frequency-Inverse Document Frequency (TF-IDF)\r\n",
    "TF-IDF is a statistical measure used to evaluate the importance of a word in a document relative to a corpus. It diminishes the weight of common words and increases the weight of rare words.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "\r\n",
    "# Sample data\r\n",
    "documents = [\"cat and dog\", \"dog and fish\", \"cat and fish\"]\r\n",
    "\r\n",
    "# Fit the TF-IDF model\r\n",
    "vectorizer = TfidfVectorizer()\r\n",
    "X = vectorizer.fit_transform(documents)\r\n",
    "\r\n",
    "print(X.toarray())\r\n",
    "print(vectorizer.get_feature_names_out())\r\n",
    "# Output: \r\n",
    "# [[0.4697916  0.58028582 0.4697916  0.4697916 ]\r\n",
    "#  [0.         0.4697916  0.58028582 0.4697916 ]\r\n",
    "#  [0.58028582 0.         0.4697916  0.58028582]]\r\n",
    "# ['and' 'cat' 'dog' 'fish']\r\n",
    "```\r\n",
    "\r\n",
    "#### 5. Word Embeddings\r\n",
    "Word embeddings represent words in a continuous vector space where semantically similar words are closer together. Methods like Word2Vec and GloVe capture word relationships and context.\r\n",
    "\r\n",
    "**Example using Gensim for Word2Vec**:\r\n",
    "```python\r\n",
    "from gensim.models import Word2Vec\r\n",
    "\r\n",
    "# Sample data\r\n",
    "sentences = [[\"cat\", \"and\", \"dog\"], [\"dog\", \"and\", \"fish\"], [\"cat\", \"and\", \"fish\"]]\r\n",
    "\r\n",
    "# Train Word2Vec model\r\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\r\n",
    "\r\n",
    "# Get vector for a word\r\n",
    "vector = model.wv['cat']\r\n",
    "print(vector)\r\n",
    "# Output: [ 0.00192769  0.00128249 -0.00423251 ... -0.00358748]\r\n",
    "```\r\n",
    "\r\n",
    "#### 6. Character-Level Embeddings\r\n",
    "Character-level embeddings represent text at the character level instead of the word level. This approach is useful for handling out-of-vocabulary words and capturing subword information.\r\n",
    "\r\n",
    "**Example using Keras for character-level embedding**:\r\n",
    "```python\r\n",
    "from keras.preprocessing.text import Tokenizer\r\n",
    "from keras.preprocessing.sequence import pad_sequences\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Embedding, Flatten, Dense\r\n",
    "\r\n",
    "# Sample data\r\n",
    "texts = [\"cat and dog\", \"dog and fish\", \"cat and fish\"]\r\n",
    "\r\n",
    "# Fit the tokenizer\r\n",
    "tokenizer = Tokenizer(char_level=True)\r\n",
    "tokenizer.fit_on_texts(texts)\r\n",
    "\r\n",
    "# Convert text to sequences\r\n",
    "sequences = tokenizer.texts_to_sequences(texts)\r\n",
    "X = pad_sequences(sequences, maxlen=15)\r\n",
    "\r\n",
    "# Define the model\r\n",
    "model = Sequential()\r\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=8, input_length=15))\r\n",
    "model.add(Flatten())\r\n",
    "model.add(Dense(1, activation='sigmoid'))\r\n",
    "\r\n",
    "print(model.summary())\r\n",
    "```\r\n",
    "\r\n",
    "#### 7. Contextualized Word Embeddings\r\n",
    "Contextualized embeddings, such as BERT and GPT, provide different representations for words based on their context in a sentence. These embeddings capture the meaning of words in different contexts more effectively than static embeddings.\r\n",
    "\r\n",
    "**Example using Hugging Face Transformers for BERT**:\r\n",
    "```python\r\n",
    "from transformers import BertTokenizer, BertModel\r\n",
    "import torch\r\n",
    "\r\n",
    "# Load pre-trained model tokenizer\r\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n",
    "\r\n",
    "# Encode text\r\n",
    "input_ids = tokenizer(\"cat and dog\", return_tensors='pt')['input_ids']\r\n",
    "\r\n",
    "# Load pre-trained model\r\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\r\n",
    "\r\n",
    "# Get embeddings\r\n",
    "with torch.no_grad():\r\n",
    "    outputs = model(input_ids)\r\n",
    "\r\n",
    "# Get the embeddings for the first token\r\n",
    "embeddings = outputs.last_hidden_state[0][0]\r\n",
    "print(embeddings)\r\n",
    "# Output: tensor([ 0.3742,  0.3469,  0.0813,  ..., -0.2564, -0.0425,  0.3590])\r\n",
    "```\r\n",
    "\r\n",
    "### Considerations in Choosing Encoding Methods\r\n",
    "\r\n",
    "1. **Vocabulary Size**:\r\n",
    "   - Large vocabularies lead to high-dimensional feature spaces in one-hot encoding and bag-of-words models.\r\n",
    "   \r\n",
    "2. **Handling Out-of-Vocabulary Words**:\r\n",
    "   - Word embeddings can handle out-of-vocabulary words better by using subword information or character-level representations.\r\n",
    "\r\n",
    "3. **Contextual Information**:\r\n",
    "   - Contextual embeddings like BERT capture the meaning of words based on their context in the sentence, providing richer representations.\r\n",
    "\r\n",
    "4. **Memory and Computation**:\r\n",
    "   - Sparse representations (like one-hot encoding) can be memory-intensive.\r\n",
    "   - Dense representations (like embeddings) are more compact and often lead to better performance in downstream tasks.\r\n",
    "\r\n",
    "5. **Domain Specificity**:\r\n",
    "   - Pre-trained embeddings (Word2Vec, GloVe) may not perform well on domain-specific vocabulary without fine-tuning.\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "Encoding is a crucial step in NLP that transforms text into numerical representations. Different encoding techniques offer various trade-offs in terms of complexity, memory usage, and the ability to capture semantic information. Understanding these techniques and choosing the appropriate one based on the task and dataset is essential for building effective NLP models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
