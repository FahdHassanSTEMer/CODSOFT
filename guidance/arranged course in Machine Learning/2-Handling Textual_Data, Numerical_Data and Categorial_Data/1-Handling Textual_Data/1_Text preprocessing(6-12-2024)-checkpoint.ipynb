{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fedbc38-8a6d-490e-bf13-47eedb6c8d00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1.nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772e95c5-10e1-42e1-86fe-b1b4afb98bf8",
   "metadata": {},
   "source": [
    "NLTK, or the Natural Language Toolkit, is a comprehensive suite of libraries and tools designed for working with human language data (text) in Python. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. Additionally, it includes wrappers for industrial-strength NLP libraries.\r\n",
    "\r\n",
    "### Key Features and Components of NLTK\r\n",
    "\r\n",
    "1. **Corpora**:\r\n",
    "   - NLTK includes a wide range of text corpora (large and structured sets of texts) for use in various natural language processing (NLP) tasks.\r\n",
    "   - Examples include the Brown Corpus, Gutenberg Corpus, and movie reviews dataset.\r\n",
    "\r\n",
    "2. **Lexical Resources**:\r\n",
    "   - Resources like WordNet, a lexical database for the English language, are available. WordNet groups English words into sets of synonyms and provides short definitions and usage examples.\r\n",
    "\r\n",
    "3. **Text Processing Libraries**:\r\n",
    "   - **Tokenization**: Splitting text into words or sentences.\r\n",
    "     ```python\r\n",
    "     from nltk.tokenize import word_tokenize, sent_tokenize\r\n",
    "     text = \"Hello world. This is a test.\"\r\n",
    "     print(word_tokenize(text))  # ['Hello', 'world', '.', 'This', 'is', 'a', 'test', '.']\r\n",
    "     print(sent_tokenize(text))  # ['Hello world.', 'This is a test.']\r\n",
    "     ```\r\n",
    "   - **Stemming**: Reducing words to their root forms.\r\n",
    "     ```python\r\n",
    "     from nltk.stem import PorterStemmer\r\n",
    "     ps = PorterStemmer()\r\n",
    "     print(ps.stem('running'))  # 'run'\r\n",
    "     ```\r\n",
    "   - **Lemmatization**: Reducing words to their base or dictionary form.\r\n",
    "     ```python\r\n",
    "     from nltk.stem import WordNetLemmatizer\r\n",
    "     lemmatizer = WordNetLemmatizer()\r\n",
    "     print(lemmatizer.lemmatize('running', pos='v'))  # 'run'\r\n",
    "     ```\r\n",
    "   - **Part-of-Speech Tagging**: Identifying the grammatical parts of speech in a sentence.\r\n",
    "     ```python\r\n",
    "     from nltk import pos_tag\r\n",
    "     words = word_tokenize(\"The quick brown fox jumps over the lazy dog\")\r\n",
    "     print(pos_tag(words))  # [('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\r\n",
    "     ```\r\n",
    "   - **Parsing**: Analyzing the grammatical structure of sentences.\r\n",
    "     ```python\r\n",
    "     from nltk import CFG\r\n",
    "     grammar = CFG.fromstring(\"\"\"\r\n",
    "     S -> NP VP\r\n",
    "     NP -> DT NN\r\n",
    "     VP -> VBZ NP\r\n",
    "     DT -> 'the'\r\n",
    "     NN -> 'dog'\r\n",
    "     VBZ -> 'barks'\r\n",
    "     \"\"\")\r\n",
    "     sentence = word_tokenize(\"the dog barks\")\r\n",
    "     parser = nltk.ChartParser(grammar)\r\n",
    "     for tree in parser.parse(sentence):\r\n",
    "         print(tree)\r\n",
    "     ```\r\n",
    "\r\n",
    "4. **Text Classification**:\r\n",
    "   - Tools for building machine learning models to classify text into categories.\r\n",
    "   - Example workflow:\r\n",
    "     - Preprocess text (tokenization, stemming/lemmatization, etc.).\r\n",
    "     - Extract features (e.g., bag of words, TF-IDF).\r\n",
    "     - Train classifiers (Naive Bayes, Decision Trees, etc.).\r\n",
    "\r\n",
    "5. **Semantic Reasoning**:\r\n",
    "   - Tools for understanding the meaning and context of words and sentences, often using lexical resources like WordNet.\r\n",
    "\r\n",
    "6. **Interfaces to Other NLP Libraries**:\r\n",
    "   - NLTK provides wrappers for using powerful NLP libraries such as Stanford NLP, OpenNLP, and SpaCy.\r\n",
    "\r\n",
    "### Installation and Basic Usage\r\n",
    "\r\n",
    "To install NLTK, you can use pip:\r\n",
    "\r\n",
    "```bash\r\n",
    "pip install nltk\r\n",
    "```\r\n",
    "\r\n",
    "Once installed, you can download the necessary datasets and models using the NLTK downloader:\r\n",
    "\r\n",
    "```python\r\n",
    "import nltk\r\n",
    "nltk.download()\r\n",
    "```\r\n",
    "\r\n",
    "This will open a graphical interface where you can choose and download specific datasets and models.\r\n",
    "\r\n",
    "### Example Workflow Using NLTK\r\n",
    "\r\n",
    "Here’s an example of a simple NLP workflow using NLTK to perform text preprocessing, feature extraction, and classification:\r\n",
    "\r\n",
    "```python\r\n",
    "import nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "# Sample data\r\n",
    "documents = [\"I love this movie\", \"I hate this movie\", \"This movie is okay\"]\r\n",
    "labels = [\"positive\", \"negative\", \"neutral\"]\r\n",
    "\r\n",
    "# Text preprocessing\r\n",
    "lemmatizer = WordNetLemmatizer()\r\n",
    "stop_words = set(stopwords.words('english'))\r\n",
    "\r\n",
    "def preprocess(text):\r\n",
    "    tokens = word_tokenize(text.lower())\r\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word.isalpha()]\r\n",
    "    return ' '.join(tokens)\r\n",
    "\r\n",
    "documents = [preprocess(doc) for doc in documents]\r\n",
    "\r\n",
    "# Feature extraction\r\n",
    "vectorizer = TfidfVectorizer()\r\n",
    "X = vectorizer.fit_transform(documents)\r\n",
    "\r\n",
    "# Train-test split\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\r\n",
    "\r\n",
    "# Train a classifier\r\n",
    "classifier = MultinomialNB()\r\n",
    "classifier.fit(X_train, y_train)\r\n",
    "\r\n",
    "# Make predictions\r\n",
    "y_pred = classifier.predict(X_test)\r\n",
    "\r\n",
    "# Evaluate the model\r\n",
    "print(classification_report(y_test, y_pred))\r\n",
    "```\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "NLTK is a powerful and flexible library for natural language processing in Python. It supports a wide range of tasks from basic text processing to advanced NLP techniques, making it a valuable tool for researchers, practitioners, and developers working with textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c0a446-355a-4cf3-8c5c-8469f878abed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2.tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d2d8d-b980-4c33-8f12-c5711bf96691",
   "metadata": {},
   "source": [
    "Tokenization is the process of splitting text into smaller units called tokens, which can be words, sentences, or subwords. It's a fundamental step in natural language processing (NLP) because it prepares raw text data for further analysis, such as parsing, part-of-speech tagging, or machine learning model training.\r\n",
    "\r\n",
    "### Types of Tokenization\r\n",
    "\r\n",
    "1. **Word Tokenization**\r\n",
    "2. **Sentence Tokenization**\r\n",
    "3. **Regular Expression Tokenization**\r\n",
    "4. **Whitespace Tokenization**\r\n",
    "\r\n",
    "### Using NLTK for Tokenization\r\n",
    "\r\n",
    "#### 1. Word Tokenization\r\n",
    "Word tokenization splits text into individual words. NLTK provides several tools for word tokenization, with `word_tokenize` being the most common.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "import nltk\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "\r\n",
    "text = \"Hello, world! This is a test.\"\r\n",
    "tokens = word_tokenize(text)\r\n",
    "print(tokens)\r\n",
    "# Output: ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', '.']\r\n",
    "```\r\n",
    "\r\n",
    "#### 2. Sentence Tokenization\r\n",
    "Sentence tokenization splits text into individual sentences. NLTK’s `sent_tokenize` function handles sentence boundaries effectively.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "from nltk.tokenize import sent_tokenize\r\n",
    "\r\n",
    "text = \"Hello, world! This is a test. NLP is fun.\"\r\n",
    "sentences = sent_tokenize(text)\r\n",
    "print(sentences)\r\n",
    "# Output: ['Hello, world!', 'This is a test.', 'NLP is fun.']\r\n",
    "```\r\n",
    "\r\n",
    "#### 3. Regular Expression Tokenization\r\n",
    "Regular expression tokenization allows for more complex and customizable tokenization patterns using regular expressions. NLTK provides `RegexpTokenizer` for this purpose.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "from nltk.tokenize import RegexpTokenizer\r\n",
    "\r\n",
    "text = \"The price of bitcoin is $42,000 as of January 2024.\"\r\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\r\n",
    "tokens = tokenizer.tokenize(text)\r\n",
    "print(tokens)\r\n",
    "# Output: ['The', 'price', 'of', 'bitcoin', 'is', '42', '000', 'as', 'of', 'January', '2024']\r\n",
    "```\r\n",
    "\r\n",
    "#### 4. Whitespace Tokenization\r\n",
    "Whitespace tokenization splits text based on whitespace characters. While not a dedicated function in NLTK, it can be easily implemented using Python’s `split` method.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "text = \"Hello, world! This is a test.\"\r\n",
    "tokens = text.split()\r\n",
    "print(tokens)\r\n",
    "# Output: ['Hello,', 'world!', 'This', 'is', 'a', 'test.']\r\n",
    "```\r\n",
    "\r\n",
    "### Tokenization Examples with NLTK\r\n",
    "\r\n",
    "#### Word Tokenization with `TreebankWordTokenizer`\r\n",
    "The `TreebankWordTokenizer` is another tokenizer provided by NLTK that uses the Penn Treebank tokenization conventions.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "from nltk.tokenize import TreebankWordTokenizer\r\n",
    "\r\n",
    "tokenizer = TreebankWordTokenizer()\r\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\r\n",
    "tokens = tokenizer.tokenize(text)\r\n",
    "print(tokens)\r\n",
    "# Output: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\r\n",
    "```\r\n",
    "\r\n",
    "#### Sentence Tokenization with `PunktSentenceTokenizer`\r\n",
    "The `PunktSentenceTokenizer` is a pre-trained unsupervised machine learning tokenizer.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "```python\r\n",
    "from nltk.tokenize import PunktSentenceTokenizer\r\n",
    "\r\n",
    "text = \"Hello, world! This is a test. NLP is fun.\"\r\n",
    "tokenizer = PunktSentenceTokenizer()\r\n",
    "sentences = tokenizer.tokenize(text)\r\n",
    "print(sentences)\r\n",
    "# Output: ['Hello, world!', 'This is a test.', 'NLP is fun.']\r\n",
    "```\r\n",
    "\r\n",
    "### Tokenization Considerations\r\n",
    "\r\n",
    "1. **Handling Punctuation**:\r\n",
    "   - In word tokenization, punctuation marks can be treated as separate tokens or be removed.\r\n",
    "   - Example: \"Hello, world!\" can be tokenized as ['Hello', ',', 'world', '!'] or ['Hello', 'world'].\r\n",
    "\r\n",
    "2. **Handling Contractions**:\r\n",
    "   - Contractions (e.g., \"don't\", \"it's\") need careful handling to split them correctly (e.g., [\"do\", \"n't\"], [\"it\", \"'s\"]).\r\n",
    "\r\n",
    "3. **Multi-language Support**:\r\n",
    "   - Tokenization rules vary across languages. NLTK supports tokenization in several languages but might require customization for specific needs.\r\n",
    "\r\n",
    "4. **Efficiency**:\r\n",
    "   - For large text corpora, efficient tokenization is crucial. NLTK’s tokenizers are generally efficient, but there are faster alternatives like SpaCy for very large datasets.\r\n",
    "\r\n",
    "### Advanced Tokenization Techniques\r\n",
    "\r\n",
    "1. **Subword Tokenization**:\r\n",
    "   - Splits text into subword units rather than words or sentences, used in models like BERT and GPT.\r\n",
    "   - **Byte Pair Encoding (BPE)** and **WordPiece Tokenization** are common subword tokenization methods.\r\n",
    "\r\n",
    "2. **Custom Tokenizers**:\r\n",
    "   - Creating custom tokenizers tailored to specific needs using regular expressions or rule-based methods.\r\n",
    "\r\n",
    "### Example: Custom Tokenizer with Regular Expressions\r\n",
    "```python\r\n",
    "from nltk.tokenize import regexp_tokenize\r\n",
    "\r\n",
    "text = \"Hello, world! This is a test.\"\r\n",
    "pattern = r'\\s|[\\.,;!\\?\\'\\\"]'\r\n",
    "tokens = regexp_tokenize(text, pattern)\r\n",
    "print(tokens)\r\n",
    "# Output: ['Hello', '', 'world', '', 'This', 'is', 'a', 'test', '']\r\n",
    "```\r\n",
    "\r\n",
    "In this example, we defined a regular expression pattern to split the text on whitespace and punctuation marks.\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "Tokenization is a crucial step in NLP that breaks down text into manageable pieces for further processing. NLTK provides a range of tokenization tools to handle different needs, from simple word and sentence tokenization to more complex regular expression-based tokenization. Understanding the characteristics of your text data and the requirements of your NLP tasks will help you choose the most appropriate tokenization method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050a7a48-d2f9-4fc1-90b2-d7d1d1be4374",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3.word rooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a51ae4-3216-4b8d-b168-a193df4785b6",
   "metadata": {},
   "source": [
    "     1-Stemming: Stemming reduces words to their base or root form, often by removing suffixes. This technique can help normalize words to their base form, which is useful in text processing tasks like search and indexing. NLTK provides several stemming algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ac9de-c226-4d61-b73b-db6854d94f9a",
   "metadata": {},
   "source": [
    "        Porter Stemmer: The Porter Stemmer is one of the most commonly used stemming algorithms. It applies a series of rules to iteratively reduce a word to its root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de5de8-5f38-4727-a54a-5c56d791841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('running'))  # Output: run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c71fc84-a2fe-4ad8-99aa-e70b6db951ed",
   "metadata": {},
   "source": [
    "        Lancaster Stemmer: The Lancaster Stemmer is more aggressive compared to the Porter Stemmer, often resulting in shorter stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8d445-6170-43fe-9ac1-8c69f682f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('running'))  # Output: run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4931563-7127-4417-adde-143fc0f73d38",
   "metadata": {},
   "source": [
    "        Snowball Stemmer: The Snowball Stemmer, also known as the Porter2 stemmer, is an improvement over the original Porter Stemmer and supports multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a4f9a-5d4e-414c-b055-0296d0aeb94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "print(stemmer.stem('running'))  # Output: run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c5f86e-fa82-41f3-a891-bf50b5aa054a",
   "metadata": {},
   "source": [
    "    2-Lemmatization:Lemmatization reduces words to their base or dictionary form (lemma) while considering the context. Unlike stemming, lemmatization uses a dictionary to map words to their lemmas, making it more accurate for understanding the morphological structure of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78837ef-1d98-47c6-9e48-1ebde77992f3",
   "metadata": {},
   "source": [
    "        WordNet Lemmatizer: The WordNet Lemmatizer uses the WordNet lexical database to find the base form of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465602ee-4eed-46b5-81a1-00ca1b12ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('running', pos='v'))  # Output: run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fce63b-788d-4f34-a770-656021e3e0bd",
   "metadata": {},
   "source": [
    "    3-Stop Words Removal: Stop words are common words that usually do not contribute significantly to the meaning of a sentence. Removing stop words helps reduce noise in the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffc35b1-9d48-471d-a72b-8df16a827ef1",
   "metadata": {},
   "source": [
    "        NLTK provides a list of common stop words for various languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea66b8d4-697c-47ae-9095-f18178a58b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "words = word_tokenize(text)\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(filtered_words)  # Output: ['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1cc44c-4bb5-4954-ad3b-ef40f35f303b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4.language recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde1a689-23ca-40d5-aeb9-66bb30e2ea33",
   "metadata": {},
   "source": [
    "    1. Parts of Speech (POS) Tagging: POS tagging involves assigning parts of speech to each word in a sentence, such as nouns, verbs, adjectives, etc. This helps in understanding the grammatical structure and syntactic relationships within the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728cd4b-b5e1-4820-b825-6af767bef2c6",
   "metadata": {},
   "source": [
    "        How POS Tagging Works in NLTK:\n",
    "            Tokenization: First, the text is tokenized into words.\n",
    "            Tagging: Each tokenized word is then tagged with a POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4fbc26-d553-4493-8698-992025448cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a sample sentence.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# POS tagging\n",
    "pos_tags = pos_tag(words)\n",
    "print(pos_tags)  # Output: [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'NN'), ('sentence', 'NN'), ('.', '.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28746141-dee7-42ae-b027-75f85f0c62a1",
   "metadata": {},
   "source": [
    "          In this example:\n",
    "            DT: Determiner\n",
    "            VBZ: Verb, 3rd person singular present\n",
    "            NN: Noun, singular or mass  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0137de13-89ce-4827-839c-4a0ed2da24ab",
   "metadata": {},
   "source": [
    "    2. Named Entity Recognition (NER): NER involves identifying and classifying named entities in text into predefined categories such as person names, organizations, locations, dates, etc. This helps in extracting important information and relationships from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd1e08-1a5d-4a8d-9c96-8ba43e52b32f",
   "metadata": {},
   "source": [
    "        How NER Works in NLTK:\n",
    "            Tokenization and POS Tagging: The text is first tokenized and POS tagged.\n",
    "            Chunking: The tagged tokens are then chunked to identify named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da50bfb7-42af-44f6-bdea-15209b586ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Sample text\n",
    "text = \"Barack Obama was born on August 4, 1961 in Honolulu, Hawaii.\"\n",
    "\n",
    "# Tokenize and POS tagging\n",
    "words = word_tokenize(text)\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "# Named Entity Recognition\n",
    "named_entities = ne_chunk(pos_tags).draw()\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61dec3e-267d-4c10-ac3b-e4ad7a0eaf65",
   "metadata": {},
   "source": [
    "        In this output:\n",
    "            PERSON: Person names\n",
    "            DATE: Dates\n",
    "            GPE: Geopolitical entities (countries, cities, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4999941a-ae72-4db9-8985-55ee55078415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
