{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d929e69-179c-44dd-8623-752014bad207",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1.word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11437f7-1a33-4f3f-ba83-39ea7827b9e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Word Embedding Explained\r\n",
    "\r\n",
    "Word embedding is a type of word representation that allows words with similar meaning to have a similar representation. It is a key component in many natural language processing (NLP) tasks, enabling algorithms to better understand and process text data. Here's a detailed explanation of word embedding:\r\n",
    "\r\n",
    "#### 1. **Concept Overview**\r\n",
    "\r\n",
    "Word embeddings map words or phrases from a vocabulary to vectors of real numbers in a low-dimensional space. Unlike traditional methods like Bag of Words (BoW) or TF-IDF, word embeddings capture the semantic meaning and relationships between words.\r\n",
    "\r\n",
    "#### 2. **Motivation for Word Embedding**\r\n",
    "\r\n",
    "Traditional methods like BoW and TF-IDF have limitations:\r\n",
    "- **High Dimensionality**: The vectors are often very sparse and high-dimensional.\r\n",
    "- **No Semantic Information**: They do not capture the meaning of words or their relationships.\r\n",
    "\r\n",
    "Word embeddings address these issues by providing dense and meaningful representations of words.\r\n",
    "\r\n",
    "#### 3. **How Word Embeddings Work**\r\n",
    "\r\n",
    "**Training Process**: Word embeddings are typically learned through neural networks by training on large text corpora. There are several popular algorithms for generating word embeddings, including Word2Vec, GloVe, and FastText.\r\n",
    "\r\n",
    "**Word2Vec**:\r\n",
    "- Developed by Google, Word2Vec includes two main architectures:\r\n",
    "  - **Continuous Bag of Words (CBOW)**: Predicts a word based on its context (neighboring words).\r\n",
    "  - **Skip-Gram**: Predicts the context (neighboring words) given a word.\r\n",
    "\r\n",
    "**GloVe (Global Vectors for Word Representation)**:\r\n",
    "- Developed by Stanford, GloVe combines global statistical information and local context to create word vectors. It uses matrix factorization techniques on the word co-occurrence matrix.\r\n",
    "\r\n",
    "**FastText**:\r\n",
    "- Developed by Facebook, FastText improves on Word2Vec by considering subword information, representing words as bags of character n-grams.\r\n",
    "\r\n",
    "**Vector Space**: The result of these algorithms is a multi-dimensional space where each word is represented as a vector. The position of the word vectors in this space reflects their semantic relationships:\r\n",
    "- **Synonyms**: Words with similar meanings are close to each other.\r\n",
    "- **Analogies**: Relationships between words can be captured, e.g., the vector representation of \"king\" - \"man\" + \"woman\" is close to \"queen\".\r\n",
    "\r\n",
    "#### 4. **Example**\r\n",
    "\r\n",
    "Suppose we use Word2Vec to generate word embeddings from a large corpus of text. The embedding space might look like this for some words:\r\n",
    "\r\n",
    "- \"king\": [0.8, 0.1, 0.3, ...]\r\n",
    "- \"queen\": [0.75, 0.12, 0.28, ...]\r\n",
    "- \"man\": [0.4, 0.05, 0.1, ...]\r\n",
    "- \"woman\": [0.35, 0.07, 0.09, ...]\r\n",
    "\r\n",
    "Here, the vector for \"king\" is close to the vector for \"queen\", and the difference between \"king\" and \"man\" is similar to the difference between \"queen\" and \"woman\".\r\n",
    "\r\n",
    "#### 5. **Applications of Word Embeddings**\r\n",
    "\r\n",
    "**1. Text Classification**: Word embeddings are used to represent words in documents, improving the performance of classification algorithms.\r\n",
    "**2. Machine Translation**: Helps in mapping words from one language to another based on their meanings.\r\n",
    "**3. Sentiment Analysis**: Captures semantic nuances that help determine the sentiment of a text.\r\n",
    "**4. Named Entity Recognition (NER)**: Assists in identifying proper nouns and other specific information in texts.\r\n",
    "**5. Information Retrieval**: Enhances search engines by understanding the meaning and context of search queries.\r\n",
    "\r\n",
    "#### 6. **Advantages of Word Embeddings**\r\n",
    "\r\n",
    "- **Semantic Representation**: Captures the meaning and relationships between words.\r\n",
    "- **Low Dimensionality**: Reduces the feature space compared to traditional methods like BoW.\r\n",
    "- **Improved Performance**: Enhances the performance of various NLP tasks by providing rich word representations.\r\n",
    "\r\n",
    "#### 7. **Disadvantages of Word Embeddings**\r\n",
    "\r\n",
    "- **Data Intensive**: Requires a large corpus of text to train effectively.\r\n",
    "- **Computationally Intensive**: Training word embeddings can be resource-heavy.\r\n",
    "- **Out-of-Vocabulary Words**: Struggles with words not seen during training, although FastText mitigates this with subword information.\r\n",
    "\r\n",
    "#### 8. **Conclusion**\r\n",
    "\r\n",
    "Word embeddings are a powerful technique in NLP that provide meaningful and dense representations of words, capturing their semantic relationships. They form the backbone of many advanced NLP applications, from text classification to machine translation. By transforming words into vectors in a continuous vector space, word embeddings enable algorithms to understand and process language more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b35404-bf2e-462b-95d7-7be1798af2c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2.Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf49778-9a1d-4623-8b49-448f9675bb98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Word2Vec Explained\r\n",
    "\r\n",
    "Word2Vec is a popular algorithm developed by Google for generating word embeddings, which are dense vector representations of words in a continuous vector space. These embeddings capture semantic relationships between words, enabling words with similar meanings to have similar vector representations. Here’s a detailed explanation of Word2Vec and how it can be used to represent semantic relations between words:\r\n",
    "\r\n",
    "#### 1. **Concept Overview**\r\n",
    "\r\n",
    "Word2Vec learns word embeddings by training on large text corpora. It uses a neural network to predict the context words given a target word (Skip-Gram) or to predict the target word from context words (CBOW - Continuous Bag of Words).\r\n",
    "\r\n",
    "**Two Main Architectures:**\r\n",
    "- **Skip-Gram Model**: Given a word, predict the surrounding context words.\r\n",
    "- **CBOW Model**: Given surrounding context words, predict the target word.\r\n",
    "\r\n",
    "#### 2. **Skip-Gram Model**\r\n",
    "\r\n",
    "**Architecture:**\r\n",
    "- The Skip-Gram model takes a single word as input and tries to predict the words that appear in its context window.\r\n",
    "- The context window is a predefined number of words surrounding the target word (e.g., 2 words to the left and 2 words to the right).\r\n",
    "\r\n",
    "**Objective Function:**\r\n",
    "- The goal is to maximize the probability of predicting context words given the target word.\r\n",
    "- For a word \\( w_t \\) in the context of words \\( w_{t-k}, \\ldots, w_{t+k} \\) (excluding \\( w_t \\)), the objective function is:\r\n",
    "  \\[\r\n",
    "  \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-k \\leq j \\leq k, j \\neq 0} \\log P(w_{t+j} | w_t)\r\n",
    "  \\]\r\n",
    "\r\n",
    "**Training Process:**\r\n",
    "- Initialize word vectors randomly.\r\n",
    "- Use stochastic gradient descent (SGD) to update word vectors based on context prediction errors.\r\n",
    "- Apply techniques like negative sampling or hierarchical softmax to efficiently approximate the softmax function used in probability computation.\r\n",
    "\r\n",
    "#### 3. **CBOW Model**\r\n",
    "\r\n",
    "**Architecture:**\r\n",
    "- The CBOW model takes the average of the embeddings of context words and uses this average to predict the target word.\r\n",
    "- It is generally faster and works well with smaller datasets compared to Skip-Gram.\r\n",
    "\r\n",
    "**Objective Function:**\r\n",
    "- The goal is to maximize the probability of predicting the target word given the context words.\r\n",
    "- For a context of words \\( w_{t-k}, \\ldots, w_{t+k} \\) (excluding \\( w_t \\)), the objective function is:\r\n",
    "  \\[\r\n",
    "  \\frac{1}{T} \\sum_{t=1}^{T} \\log P(w_t | w_{t-k}, \\ldots, w_{t-1}, w_{t+1}, \\ldots, w_{t+k})\r\n",
    "  \\]\r\n",
    "\r\n",
    "**Training Process:**\r\n",
    "- Similar to Skip-Gram, use SGD to update the word vectors based on the prediction errors.\r\n",
    "\r\n",
    "#### 4. **Mathematical Details**\r\n",
    "\r\n",
    "**Word Probability Calculation:**\r\n",
    "- The probability \\( P(w_{context} | w_{target}) \\) is computed using the softmax function:\r\n",
    "  \\[\r\n",
    "  P(w_O | w_I) = \\frac{\\exp(\\mathbf{v}_O \\cdot \\mathbf{v}_I)}{\\sum_{w=1}^{W} \\exp(\\mathbf{v}_w \\cdot \\mathbf{v}_I)}\r\n",
    "  \\]\r\n",
    "  where \\( \\mathbf{v}_O \\) is the output vector of the context word, \\( \\mathbf{v}_I \\) is the input vector of the target word, and \\( W \\) is the vocabulary size.\r\n",
    "\r\n",
    "**Negative Sampling:**\r\n",
    "- To make the computation efficient, negative sampling approximates the softmax function by only updating a small sample of negative examples for each positive example.\r\n",
    "\r\n",
    "#### 5. **Using Word2Vec for Semantic Relationships**\r\n",
    "\r\n",
    "Once trained, the word vectors can be used to find semantic relationships between words. Here’s how:\r\n",
    "\r\n",
    "**1. Cosine Similarity:**\r\n",
    "- Measure the similarity between words using cosine similarity. Words with high cosine similarity are considered semantically similar.\r\n",
    "  \\[\r\n",
    "  \\text{cosine similarity}(\\mathbf{v}_1, \\mathbf{v}_2) = \\frac{\\mathbf{v}_1 \\cdot \\mathbf{v}_2}{\\|\\mathbf{v}_1\\| \\|\\mathbf{v}_2\\|}\r\n",
    "  \\]\r\n",
    "\r\n",
    "**2. Vector Arithmetic:**\r\n",
    "- Perform vector arithmetic to capture analogies. For example, \"king\" - \"man\" + \"woman\" ≈ \"queen\". This is because the relationships between words are preserved in the vector space.\r\n",
    "\r\n",
    "**3. Nearest Neighbors:**\r\n",
    "- Find the nearest neighbors of a word in the vector space to discover words with similar meanings.\r\n",
    "\r\n",
    "#### 6. **Implementation with Gensim**\r\n",
    "\r\n",
    "Gensim is a popular library for training and using Word2Vec models. Here’s a basic implementation:\r\n",
    "\r\n",
    "```python\r\n",
    "from gensim.models import Word2Vec\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "import nltk\r\n",
    "\r\n",
    "# Download NLTK data (only the first time)\r\n",
    "nltk.download('punkt')\r\n",
    "\r\n",
    "# Sample corpus\r\n",
    "corpus = [\r\n",
    "    \"the cat sat on the mat\",\r\n",
    "    \"the dog sat on the mat\",\r\n",
    "    \"the cat ate the food\",\r\n",
    "    \"the dog ate the food\",\r\n",
    "]\r\n",
    "\r\n",
    "# Tokenize the corpus\r\n",
    "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\r\n",
    "\r\n",
    "# Train Word2Vec model\r\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=2, min_count=1, sg=0)  # CBOW\r\n",
    "# model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=2, min_count=1, sg=1)  # Skip-Gram\r\n",
    "\r\n",
    "# Get the vector for a word\r\n",
    "vector = model.wv['cat']\r\n",
    "\r\n",
    "# Find similar words\r\n",
    "similar_words = model.wv.most_similar('cat')\r\n",
    "\r\n",
    "print(\"Vector for 'cat':\", vector)\r\n",
    "print(\"Words similar to 'cat':\", similar_words)\r\n",
    "```\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "Word2Vec is a powerful technique for generating word embeddings that capture semantic relationships between words. By using neural network architectures like Skip-Gram and CBOW, Word2Vec learns word representations that preserve contextual similarities and relationships. These embeddings can be used in various NLP tasks, providing a meaningful and dense representation of words that traditional methods lack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2444927b-ff1c-4d8e-a51e-4386289cbcd2",
   "metadata": {},
   "source": [
    "### Relation of Word2Vec to Feature Representation\r\n",
    "\r\n",
    "Word2Vec plays a significant role in feature representation, particularly in the field of natural language processing (NLP). Here's how Word2Vec relates to and enhances feature representation:\r\n",
    "\r\n",
    "#### 1. **Feature Representation Overview**\r\n",
    "\r\n",
    "**Feature Representation** in machine learning refers to the process of converting raw data into a format (features) that can be used by machine learning algorithms. For textual data, this involves converting words and documents into numerical vectors.\r\n",
    "\r\n",
    "Traditional methods for feature representation in NLP include:\r\n",
    "- **Bag of Words (BoW)**: Represents text as a collection of word counts or frequencies, resulting in high-dimensional and sparse vectors.\r\n",
    "- **TF-IDF (Term Frequency-Inverse Document Frequency)**: Weights words by their importance, still resulting in high-dimensional vectors.\r\n",
    "\r\n",
    "#### 2. **Word2Vec as a Feature Representation Method**\r\n",
    "\r\n",
    "Word2Vec transforms words into dense, low-dimensional vectors that capture semantic meanings and relationships between words. This process significantly enhances feature representation for textual data.\r\n",
    "\r\n",
    "**Key Characteristics of Word2Vec Feature Representation:**\r\n",
    "- **Dense Vectors**: Word2Vec creates dense vectors where each word is represented by a vector of fixed dimensions (e.g., 100 or 300 dimensions).\r\n",
    "- **Semantic Similarity**: Words with similar meanings or contexts have similar vectors. This captures the semantic relationships between words, unlike traditional methods which treat words independently.\r\n",
    "- **Low Dimensionality**: The vectors are of lower dimensionality compared to the vocabulary size, making computations more efficient.\r\n",
    "\r\n",
    "#### 3. **Advantages of Word2Vec for Feature Representation**\r\n",
    "\r\n",
    "**1. Semantic Meaning**:\r\n",
    "- Word2Vec captures semantic relationships, meaning words like \"king\" and \"queen\" or \"man\" and \"woman\" are represented in a way that their vectors reflect their semantic similarities and relationships.\r\n",
    "\r\n",
    "**2. Dimensionality Reduction**:\r\n",
    "- Compared to methods like BoW or TF-IDF, Word2Vec significantly reduces the dimensionality of the feature space, making it more computationally efficient while retaining meaningful information.\r\n",
    "\r\n",
    "**3. Context Awareness**:\r\n",
    "- Word2Vec takes the context of words into account during training, resulting in vectors that capture the context in which words appear.\r\n",
    "\r\n",
    "**4. Vector Arithmetic**:\r\n",
    "- The ability to perform vector arithmetic (e.g., \"king\" - \"man\" + \"woman\" ≈ \"queen\") provides powerful ways to explore and use semantic relationships.\r\n",
    "\r\n",
    "#### 4. **Applications of Word2Vec Feature Representation**\r\n",
    "\r\n",
    "**1. Text Classification**:\r\n",
    "- Word vectors can be averaged or pooled to represent entire documents or sentences, which can then be used as features for classification tasks.\r\n",
    "\r\n",
    "**2. Clustering**:\r\n",
    "- Similarity between word vectors allows for effective clustering of similar words or documents, aiding in tasks like topic modeling.\r\n",
    "\r\n",
    "**3. Information Retrieval**:\r\n",
    "- Enhanced search capabilities by matching query terms with semantically similar terms in documents.\r\n",
    "\r\n",
    "**4. Sentiment Analysis**:\r\n",
    "- Improved sentiment analysis by understanding the context and meaning of words in a text.\r\n",
    "\r\n",
    "**5. Named Entity Recognition (NER)**:\r\n",
    "- Better identification and classification of proper nouns and other entities in texts due to the rich semantic information in word vectors.\r\n",
    "\r\n",
    "#### 5. **Practical Implementation of Word2Vec**\r\n",
    "\r\n",
    "To use Word2Vec for feature representation in a practical scenario, follow these steps:\r\n",
    "\r\n",
    "**Step 1: Train Word2Vec Model** (or use pre-trained embeddings)\r\n",
    "```python\r\n",
    "from gensim.models import Word2Vec\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "import nltk\r\n",
    "\r\n",
    "# Download NLTK data (only the first time)\r\n",
    "nltk.download('punkt')\r\n",
    "\r\n",
    "# Sample corpus\r\n",
    "corpus = [\r\n",
    "    \"the cat sat on the mat\",\r\n",
    "    \"the dog sat on the mat\",\r\n",
    "    \"the cat ate the food\",\r\n",
    "    \"the dog ate the food\",\r\n",
    "]\r\n",
    "\r\n",
    "# Tokenize the corpus\r\n",
    "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\r\n",
    "\r\n",
    "# Train Word2Vec model\r\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=2, min_count=1, sg=0)  # CBOW\r\n",
    "# model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=2, min_count=1, sg=1)  # Skip-Gram\r\n",
    "\r\n",
    "# Get the vector for a word\r\n",
    "vector = model.wv['cat']\r\n",
    "\r\n",
    "print(\"Vector for 'cat':\", vector)\r\n",
    "```\r\n",
    "\r\n",
    "**Step 2: Use Word Vectors as Features**\r\n",
    "- For a text classification task, represent each document by averaging the word vectors of the words it contains.\r\n",
    "\r\n",
    "```python\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "def document_vector(doc, model):\r\n",
    "    # Remove out-of-vocabulary words\r\n",
    "    doc = [word for word in doc if word in model.wv]\r\n",
    "    return np.mean(model.wv[doc], axis=0)\r\n",
    "\r\n",
    "# Example: Represent a new document\r\n",
    "new_doc = word_tokenize(\"the cat sat on the mat\")\r\n",
    "new_doc_vector = document_vector(new_doc, model)\r\n",
    "print(\"Document vector:\", new_doc_vector)\r\n",
    "```\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "Word2Vec provides a powerful method for feature representation by transforming words into dense, semantically meaningful vectors. This enhances the ability of machine learning models to understand and process textual data, leading to better performance in various NLP tasks. The relationship between Word2Vec and feature representation lies in its ability to capture and encode semantic relationships in a low-dimensional space, offering significant advantages over traditional methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d2aa5-1e8b-43ea-b909-066bdae47b16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3.Relation between TF-IDF and Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6744f26-62f2-43f7-9442-d7c6ce0aaf40",
   "metadata": {},
   "source": [
    "### Relation Between TF-IDF and Word2Vec\r\n",
    "\r\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) and Word2Vec are both techniques used in natural language processing (NLP) to represent text data, but they are fundamentally different in their approaches and the type of information they capture. Here’s a detailed comparison and explanation of their relationship:\r\n",
    "\r\n",
    "#### 1. **Concept Overview**\r\n",
    "\r\n",
    "**TF-IDF**:\r\n",
    "- **Purpose**: TF-IDF is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus).\r\n",
    "- **Components**:\r\n",
    "  - **Term Frequency (TF)**: Measures how frequently a term appears in a document.\r\n",
    "  - **Inverse Document Frequency (IDF)**: Measures how important a term is by considering how often it appears across all documents in the corpus. Less frequent terms are considered more important.\r\n",
    "  - The TF-IDF score for a term \\( t \\) in a document \\( d \\) is computed as:\r\n",
    "    \\[\r\n",
    "    \\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\r\n",
    "    \\]\r\n",
    "  - **High-dimensional Sparse Representation**: Each document is represented as a sparse vector where the dimensionality equals the number of unique terms in the corpus.\r\n",
    "\r\n",
    "**Word2Vec**:\r\n",
    "- **Purpose**: Word2Vec is a neural network-based technique that generates dense vector representations (embeddings) for words. These vectors capture semantic relationships between words.\r\n",
    "- **Training Objectives**:\r\n",
    "  - **Skip-Gram**: Predicts the context words given a target word.\r\n",
    "  - **CBOW (Continuous Bag of Words)**: Predicts the target word given the context words.\r\n",
    "- **Low-dimensional Dense Representation**: Each word is represented as a dense vector in a lower-dimensional space (e.g., 100 or 300 dimensions).\r\n",
    "\r\n",
    "#### 2. **Differences Between TF-IDF and Word2Vec**\r\n",
    "\r\n",
    "**1. Representation Type**:\r\n",
    "- **TF-IDF**: Produces sparse vectors where each dimension represents a term from the corpus.\r\n",
    "- **Word2Vec**: Produces dense vectors where each dimension represents a latent feature capturing semantic information.\r\n",
    "\r\n",
    "**2. Information Captured**:\r\n",
    "- **TF-IDF**: Focuses on the importance of terms based on their frequency across documents. It does not capture semantic relationships between words.\r\n",
    "- **Word2Vec**: Captures semantic relationships between words by placing similar words close to each other in the vector space.\r\n",
    "\r\n",
    "**3. Dimensionality**:\r\n",
    "- **TF-IDF**: High-dimensional, equal to the number of unique terms in the corpus.\r\n",
    "- **Word2Vec**: Low-dimensional, typically pre-defined (e.g., 100 or 300 dimensions).\r\n",
    "\r\n",
    "**4. Context Consideration**:\r\n",
    "- **TF-IDF**: Considers only the frequency of terms without context.\r\n",
    "- **Word2Vec**: Considers the context in which words appear, learning embeddings based on neighboring words.\r\n",
    "\r\n",
    "#### 3. **Complementary Use of TF-IDF and Word2Vec**\r\n",
    "\r\n",
    "Although TF-IDF and Word2Vec are different, they can complement each other in various NLP tasks. Here’s how they can be used together:\r\n",
    "\r\n",
    "**1. Hybrid Feature Representation**:\r\n",
    "- Combine TF-IDF and Word2Vec vectors to form a hybrid representation. This can be beneficial for certain tasks where both term frequency information and semantic relationships are important.\r\n",
    "\r\n",
    "```python\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from gensim.models import Word2Vec\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# Sample corpus\r\n",
    "corpus = [\r\n",
    "    \"the cat sat on the mat\",\r\n",
    "    \"the dog sat on the mat\",\r\n",
    "    \"the cat ate the food\",\r\n",
    "    \"the dog ate the food\",\r\n",
    "]\r\n",
    "\r\n",
    "# Tokenize the corpus\r\n",
    "tokenized_corpus = [doc.split() for doc in corpus]\r\n",
    "\r\n",
    "# Train Word2Vec model\r\n",
    "word2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=2, min_count=1, sg=0)\r\n",
    "\r\n",
    "# Create TF-IDF vectors\r\n",
    "tfidf_vectorizer = TfidfVectorizer()\r\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus).toarray()\r\n",
    "\r\n",
    "# Function to combine TF-IDF and Word2Vec vectors\r\n",
    "def combine_tfidf_word2vec(tfidf_matrix, word2vec_model, tokenized_corpus):\r\n",
    "    combined_vectors = []\r\n",
    "    for i, doc in enumerate(tokenized_corpus):\r\n",
    "        doc_vector = np.zeros(word2vec_model.vector_size)\r\n",
    "        for word in doc:\r\n",
    "            if word in word2vec_model.wv:\r\n",
    "                tfidf_index = tfidf_vectorizer.vocabulary_.get(word)\r\n",
    "                if tfidf_index is not None:\r\n",
    "                    doc_vector += tfidf_matrix[i][tfidf_index] * word2vec_model.wv[word]\r\n",
    "        combined_vectors.append(doc_vector)\r\n",
    "    return np.array(combined_vectors)\r\n",
    "\r\n",
    "# Combine vectors\r\n",
    "combined_vectors = combine_tfidf_word2vec(tfidf_matrix, word2vec_model, tokenized_corpus)\r\n",
    "print(combined_vectors)\r\n",
    "```\r\n",
    "\r\n",
    "**2. Feature Engineering**:\r\n",
    "- Use TF-IDF to select important words and then use Word2Vec to create embeddings only for those selected words. This reduces noise and focuses on important terms.\r\n",
    "\r\n",
    "**3. Model Input**:\r\n",
    "- For some machine learning models, combining both types of vectors as input features can improve performance. For example, using TF-IDF vectors for capturing term importance and Word2Vec vectors for capturing semantic meaning.\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "TF-IDF and Word2Vec are distinct methods for feature representation in NLP, each with its strengths and weaknesses. TF-IDF captures the importance of terms based on their frequency across documents, while Word2Vec captures semantic relationships between words in a dense vector space. Combining these methods can provide richer and more informative feature representations, leveraging the strengths of both approaches for improved performance in various NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253d727e-a6c1-4487-b2c8-524d0edbd7b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4.Neural Network (How to create CBOW to interper semantic relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3863b5-b655-4897-8afa-c7c6b6602d21",
   "metadata": {},
   "source": [
    "If you're looking for a straightforward course to get you started with the basics of Artificial Neural Networks (ANN), loss functions, and optimizers, I recommend starting with a beginner-friendly course that covers the fundamentals of deep learning. Here are a few options that are well-suited for beginners:\n",
    "\n",
    "### 1. **\"Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning\" by Coursera**\n",
    "\n",
    "- **Description**: This course, offered by deeplearning.ai, is designed for beginners and covers the basics of TensorFlow, including building simple neural networks. It introduces key concepts such as ANNs, loss functions, and optimizers.\n",
    "- **Instructor**: Laurence Moroney\n",
    "- **Duration**: Approximately 4 weeks, 4-5 hours per week\n",
    "- **Link**: [Introduction to TensorFlow for AI, ML, and DL](https://www.coursera.org/learn/introduction-tensorflow)\n",
    "\n",
    "### 2. **\"Deep Learning Fundamentals\" by IBM on Coursera**\n",
    "\n",
    "- **Description**: This course is part of IBM's AI Engineering Professional Certificate. It introduces the fundamental concepts of deep learning, including ANNs, loss functions, and optimizers, in an easy-to-understand manner.\n",
    "- **Instructor**: Rav Ahuja\n",
    "- **Duration**: Approximately 5 weeks, 2-4 hours per week\n",
    "- **Link**: [Deep Learning Fundamentals](https://www.coursera.org/learn/deep-neural-networks-with-pytorch)\n",
    "\n",
    "### 3. **\"Neural Networks and Deep Learning\" by Coursera**\n",
    "\n",
    "- **Description**: This course is part of the Deep Learning Specialization by Andrew Ng. The first course in the specialization, it covers the basics of neural networks, the concept of a loss function, and the optimization process.\n",
    "- **Instructor**: Andrew Ng\n",
    "- **Duration**: Approximately 4 weeks, 3-4 hours per week\n",
    "- **Link**: [Neural Networks and Deep Learning](https://www.coursera.org/learn/neural-networks-deep-learning)\n",
    "\n",
    "### 4. **\"Deep Learning with Python and PyTorch\" by Udacity**\n",
    "\n",
    "- **Description**: This free course is aimed at beginners and covers the basics of neural networks using Python and PyTorch. It includes explanations of ANNs, loss functions, and optimizers.\n",
    "- **Duration**: Self-paced, approximately 4 weeks\n",
    "- **Link**: [Deep Learning with Python and PyTorch](https://www.udacity.com/course/deep-learning-pytorch--ud188)\n",
    "\n",
    "### 5. **\"Intro to Machine Learning with PyTorch\" by Udacity**\n",
    "\n",
    "- **Description**: While this course covers a broader range of machine learning topics, it includes essential modules on neural networks, loss functions, and optimizers. It's designed for beginners and provides practical, hands-on learning.\n",
    "- **Duration**: Approximately 2 months, 10 hours per week\n",
    "- **Link**: [Intro to Machine Learning with PyTorch](https://www.udacity.com/course/intro-to-machine-learning-with-pytorch--ud188)\n",
    "\n",
    "### Summary\n",
    "\n",
    "These courses are designed to be accessible to beginners with little to no prior experience in machine learning or deep learning. They will help you understand the key concepts of ANNs, loss functions, and optimizers through simple explanations and practical examples. I recommend starting with one of these courses to build a strong foundation in these essential topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd45833d-455b-4841-a2b8-6b0ca7d82415",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5.CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc182fe-15d6-45bb-a274-7269326f9346",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Continuous Bag of Words (CBOW) is one of the two model architectures introduced by Tomas Mikolov et al. in 2013 as part of the Word2Vec family for generating word embeddings. The other architecture is Skip-gram. CBOW aims to predict a target word given its context words, and it is particularly efficient for handling large datasets.\r\n",
    "\r\n",
    "### Overview of CBOW\r\n",
    "\r\n",
    "The primary objective of CBOW is to predict a word given its surrounding context words in a text. It assumes that words appearing in similar contexts have similar meanings and thus should have similar vector representations.\r\n",
    "\r\n",
    "### Key Concepts\r\n",
    "\r\n",
    "1. **Context and Target**: In CBOW, the context consists of the words surrounding a target word within a specified window size. For example, in the sentence \"The quick brown fox jumps over the lazy dog\" with a window size of 2, the context for the word \"brown\" would be [\"The\", \"quick\", \"fox\", \"jumps\"].\r\n",
    "   \r\n",
    "2. **Word Vectors**: Each word in the vocabulary is represented by a continuous vector. These vectors are learned during the training process.\r\n",
    "\r\n",
    "3. **Window Size**: This parameter defines the number of words to consider to the left and right of the target word. A larger window size captures more context but may also include irrelevant words.\r\n",
    "\r\n",
    "### CBOW Model Architecture\r\n",
    "\r\n",
    "The CBOW model architecture consists of three layers: an input layer, a hidden layer, and an output layer.\r\n",
    "\r\n",
    "1. **Input Layer**: The input to the CBOW model is a set of context words represented as one-hot vectors. If the vocabulary size is \\( V \\), each context word is a \\( V \\)-dimensional one-hot vector.\r\n",
    "\r\n",
    "2. **Hidden Layer**: The hidden layer is a weight matrix \\( W \\) of size \\( V \\times N \\), where \\( N \\) is the dimensionality of the word vectors. The hidden layer computes the average of the input context word vectors.\r\n",
    "\r\n",
    "   \\( h = \\frac{1}{C} \\sum_{i=1}^{C} W_{i} \\)\r\n",
    "\r\n",
    "   where \\( C \\) is the number of context words and \\( W_i \\) is the one-hot vector of the \\( i \\)-th context word.\r\n",
    "\r\n",
    "3. **Output Layer**: The output layer is another weight matrix \\( W' \\) of size \\( N \\times V \\). The output layer produces a probability distribution over the vocabulary using the softmax function.\r\n",
    "\r\n",
    "   \\( p(w_O | w_I) = \\frac{\\exp(W'_{w_O} \\cdot h)}{\\sum_{i=1}^{V} \\exp(W'_i \\cdot h)} \\)\r\n",
    "\r\n",
    "   where \\( w_O \\) is the target word, and \\( W'_i \\) are the weights corresponding to each word in the vocabulary.\r\n",
    "\r\n",
    "### Training the CBOW Model\r\n",
    "\r\n",
    "Training the CBOW model involves adjusting the weights in the hidden and output layers to maximize the probability of the target word given the context words. This is done using stochastic gradient descent (SGD) and backpropagation.\r\n",
    "\r\n",
    "1. **Initialization**: Initialize the weight matrices \\( W \\) and \\( W' \\) with small random values.\r\n",
    "\r\n",
    "2. **Forward Pass**: Compute the hidden layer representation by averaging the context word vectors, then compute the output probabilities using the softmax function.\r\n",
    "\r\n",
    "3. **Loss Function**: Use the cross-entropy loss to measure the difference between the predicted probabilities and the actual target word.\r\n",
    "\r\n",
    "4. **Backward Pass**: Compute the gradients of the loss with respect to the weights and update the weights using gradient descent.\r\n",
    "\r\n",
    "### Example Implementation of CBOW using Gensim\r\n",
    "\r\n",
    "Here's a basic example of how to implement and train a CBOW model using the Gensim library in Python.\r\n",
    "\r\n",
    "```python\r\n",
    "import gensim\r\n",
    "from gensim.models import Word2Vec\r\n",
    "from nltk.corpus import brown\r\n",
    "\r\n",
    "# Download the Brown corpus\r\n",
    "import nltk\r\n",
    "nltk.download('brown')\r\n",
    "nltk.download('punkt')\r\n",
    "\r\n",
    "# Prepare the dataset\r\n",
    "sentences = brown.sents()\r\n",
    "\r\n",
    "# Train a CBOW model\r\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4, sg=0) # sg=0 means CBOW\r\n",
    "\r\n",
    "# Get the vector for a word\r\n",
    "vector = model.wv['quick']\r\n",
    "print(vector)\r\n",
    "\r\n",
    "# Find most similar words\r\n",
    "similar_words = model.wv.most_similar('quick')\r\n",
    "print(similar_words)\r\n",
    "```\r\n",
    "\r\n",
    "### Advantages and Disadvantages of CBOW\r\n",
    "\r\n",
    "#### Advantages\r\n",
    "1. **Efficiency**: CBOW is computationally more efficient than Skip-gram because it predicts a single word from multiple context words.\r\n",
    "2. **Simplicity**: The model architecture and training procedure are straightforward to implement and understand.\r\n",
    "\r\n",
    "#### Disadvantages\r\n",
    "1. **Suboptimal for Rare Words**: CBOW tends to underperform for rare words since it relies on the context to predict the target word.\r\n",
    "2. **Context Information Loss**: Averaging context words may lead to loss of important positional information and relationships between words.\r\n",
    "\r\n",
    "### Applications of CBOW\r\n",
    "\r\n",
    "1. **Text Similarity**: CBOW-generated word embeddings can be used to measure text similarity by comparing the average vectors of the texts.\r\n",
    "2. **Text Classification**: Word embeddings can be used as features for training classifiers to categorize text into predefined labels.\r\n",
    "3. **Sentiment Analysis**: Word embeddings can be used to capture sentiment by understanding the context of words in the text.\r\n",
    "4. **Information Retrieval**: Word embeddings can improve search engines by understanding synonyms and related terms.\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "CBOW is a fundamental technique in NLP for generating word embeddings. It predicts a target word based on its surrounding context words, making it computationally efficient and effective for various NLP tasks. While it has some limitations, such as handling rare words and context information loss, it remains a valuable tool in the NLP toolkit. Understanding CBOW provides a foundation for exploring more advanced models and techniques in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e13105f-9ae8-4c88-bb22-bf1b6f031f3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 6.Avg word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9fb954-eccc-421e-a2f5-1da1d5ffd582",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Average Word2Vec in Detail\r\n",
    "\r\n",
    "**Average Word2Vec** is a technique used to create fixed-size feature representations for entire texts (such as sentences, paragraphs, or documents) by averaging the word vectors of the individual words within the text. This method leverages pre-trained word embeddings, such as those generated by Word2Vec, to represent the words in a high-dimensional vector space. Here’s a detailed explanation of the process and its practical implementations:\r\n",
    "\r\n",
    "#### Step-by-Step Process\r\n",
    "\r\n",
    "1. **Train or Obtain Pre-trained Word2Vec Embeddings**:\r\n",
    "   - You can either train your own Word2Vec model on a large corpus of text using tools like Gensim or TensorFlow or use pre-trained embeddings like Google's Word2Vec or those available through libraries like Gensim.\r\n",
    "   - Each word in the vocabulary is represented as a dense vector in a high-dimensional space (e.g., 100, 200, or 300 dimensions).\r\n",
    "\r\n",
    "2. **Tokenize the Text**:\r\n",
    "   - Split the text into individual words (tokens). This step typically involves lowercasing, removing punctuation, and splitting by spaces.\r\n",
    "   - Example: For the sentence \"The cat sat on the mat,\" the tokens would be [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"].\r\n",
    "\r\n",
    "3. **Retrieve Word Vectors**:\r\n",
    "   - For each tokenized word, retrieve its corresponding word vector from the pre-trained Word2Vec model.\r\n",
    "   - If a word is not in the vocabulary of the Word2Vec model, it can either be ignored or replaced with a zero vector (or another strategy).\r\n",
    "\r\n",
    "4. **Compute the Average of Word Vectors**:\r\n",
    "   - Calculate the element-wise average of the word vectors to obtain a single vector representation for the entire text.\r\n",
    "   - This involves summing the vectors for each word and then dividing by the number of words.\r\n",
    "   - Mathematically, for a text with \\( n \\) words \\( w_1, w_2, ..., w_n \\) and their corresponding word vectors \\( \\vec{v_1}, \\vec{v_2}, ..., \\vec{v_n} \\):\r\n",
    "     \\[\r\n",
    "     \\text{average\\_vector} = \\frac{1}{n} \\sum_{i=1}^{n} \\vec{v_i}\r\n",
    "     ```\r\n",
    "\r\n",
    "#### Example\r\n",
    "\r\n",
    "Let's consider an example sentence: \"The cat sat on the mat.\"\r\n",
    "\r\n",
    "1. **Tokenize the Sentence**:\r\n",
    "   ```python\r\n",
    "   sentence = \"The cat sat on the mat\"\r\n",
    "   tokens = sentence.lower().split()  # ['the', 'cat', 'sat', 'on', 'the', 'mat']\r\n",
    "   ```\r\n",
    "\r\n",
    "2. **Retrieve Word Vectors**:\r\n",
    "   Assume we have a pre-trained Word2Vec model loaded in `model`. We retrieve the vectors for each word.\r\n",
    "\r\n",
    "   ```python\r\n",
    "   import numpy as np\r\n",
    "\r\n",
    "   word_vectors = [model[word] for word in tokens if word in model]\r\n",
    "   ```\r\n",
    "\r\n",
    "3. **Compute the Average Vector**:\r\n",
    "   ```python\r\n",
    "   average_vector = np.mean(word_vectors, axis=0)\r\n",
    "   ```\r\n",
    "\r\n",
    "#### Benefits and Limitations\r\n",
    "\r\n",
    "**Benefits**:\r\n",
    "1. **Simplicity**: Easy to implement and understand.\r\n",
    "2. **Fixed-Length Representation**: Provides a fixed-size vector regardless of the input text length, which is useful for machine learning models.\r\n",
    "3. **Leverages Pre-trained Embeddings**: Utilizes the semantic information captured by pre-trained word vectors.\r\n",
    "\r\n",
    "**Limitations**:\r\n",
    "1. **Loss of Context**: Averages the word vectors, potentially losing the context of individual words (e.g., word order, syntax).\r\n",
    "2. **Out-of-Vocabulary Words**: Words not in the pre-trained model's vocabulary need special handling.\r\n",
    "3. **Ineffective for Long Texts**: May not capture the nuances of longer texts as effectively as more sophisticated methods like recurrent neural networks (RNNs) or transformers.\r\n",
    "\r\n",
    "#### Practical Implementations\r\n",
    "\r\n",
    "**1. Text Classification**:\r\n",
    "   - **Spam Detection**: Email or message text can be represented as an average word vector and fed into a classifier to determine whether it is spam or not.\r\n",
    "   - **Sentiment Analysis**: Customer reviews or social media posts can be classified as positive, negative, or neutral based on their average word vector representations.\r\n",
    "\r\n",
    "**2. Clustering**:\r\n",
    "   - **Document Clustering**: Articles, news reports, or research papers can be grouped into clusters based on the similarity of their average word vectors.\r\n",
    "   - **Topic Modeling**: Grouping texts into topics by clustering their average word vectors to find underlying themes.\r\n",
    "\r\n",
    "**3. Semantic Similarity**:\r\n",
    "   - **Plagiarism Detection**: Comparing the average word vectors of documents to detect similarities.\r\n",
    "   - **Duplicate Detection**: Identifying duplicate questions in forums or duplicate entries in databases by comparing average word vectors.\r\n",
    "\r\n",
    "**4. Information Retrieval**:\r\n",
    "   - **Search Engines**: Enhancing search result relevance by comparing the query's average word vector to the average word vectors of documents in the database.\r\n",
    "   - **Recommender Systems**: Recommending articles or products based on the similarity of average word vectors to the user's interests.\r\n",
    "\r\n",
    "**5. Feature Engineering**:\r\n",
    "   - **Machine Learning Models**: Using average word vectors as input features for traditional machine learning models like logistic regression, SVM, or gradient boosting machines.\r\n",
    "\r\n",
    "**6. Anomaly Detection**:\r\n",
    "   - **Fraud Detection**: Analyzing transaction descriptions by representing them as average word vectors to detect anomalies or fraudulent behavior.\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "Average Word2Vec is a straightforward and effective method to create a single vector representation for a piece of text by averaging the word embeddings of its constituent words. It is valuable for many NLP tasks due to its simplicity and efficiency, with practical applications in text classification, clustering, semantic similarity, information retrieval, feature engineering, and anomaly detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
